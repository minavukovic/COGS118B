{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ppokhare/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import inflect\n",
    "import re\n",
    "#!pip install contractions --user\n",
    "import contractions\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nuc24nwLI46K"
   },
   "outputs": [],
   "source": [
    "#Loading and Cleaning Data\n",
    "\n",
    "#load in the csv contatining the data\n",
    "df_original = pd.read_csv('wiki_movie_plots_deduped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpVQi0yQI9fy"
   },
   "outputs": [],
   "source": [
    "#drop all rows in the dataframe that do not contain horror(1167) in genre catergory\n",
    "df_horror = df_original.drop(df_original[df_original['Genre'] != 'horror'].index,inplace=False)\n",
    "#drop columns we wont need\n",
    "df_horror.drop(['Release Year','Origin/Ethnicity','Cast', 'Wiki Page','Director','Release Year'],axis=1, \n",
    "               inplace = True)\n",
    "\n",
    "#reduce horror to 1000 rows\n",
    "df_horror = df_horror.sample(frac=1)\n",
    "df_horror = df_horror.reset_index(drop=True)\n",
    "df_horror_train = df_horror[0:1000]\n",
    "df_horror_test = df_horror[1000:len(df_horror)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAwh4SxaJF0s"
   },
   "outputs": [],
   "source": [
    "#drop all rows in dataframe that do not contain comedy(4379)\n",
    "df_comedy = df_original.drop(df_original[df_original['Genre'] != 'comedy'].index, inplace = False)\n",
    "df_comedy.drop(['Release Year','Origin/Ethnicity','Cast', 'Wiki Page','Director','Release Year'],axis=1, \n",
    "               inplace = True)\n",
    "#reduce horror to 1000 rows\n",
    "df_comedy = df_comedy.sample(frac=1)\n",
    "df_comedy = df_comedy.reset_index(drop=True)\n",
    "df_comedy_train = df_comedy[0:1000]\n",
    "df_comedy_test = df_comedy[1000:len(df_comedy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4-RZfoyJUvr"
   },
   "outputs": [],
   "source": [
    "#merge the comedy and horror dataframes into one\n",
    "df_HandC = pd.concat([df_horror_test,df_comedy_test])\n",
    "\n",
    "#Pre punctuation parsing test\n",
    "#df_HandC.iloc[1002]['Plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for text pre-processing\n",
    "def normalizeText(row):\n",
    "    \n",
    "    #create a list of punctuations we wish to delete\n",
    "    punctuations = string.punctuation\n",
    "    #create a list of stopwords in English\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    #change all letters to lower case\n",
    "    row = row.lower()  \n",
    "    \n",
    "    #remove numbers\n",
    "    row = re.sub(\" \\d+\", \" \", row)\n",
    "                 \n",
    "    #remove punctutation\n",
    "    for letter in row: \n",
    "    \n",
    "        if letter in punctuations: \n",
    "            row = row.replace(letter, \"\")\n",
    "    \n",
    "    #expand the contraction I'm -> I am\n",
    "    row = contractions.fix(row)\n",
    "    # got it from (https://github.com/kootenpv/contractions)\n",
    "                \n",
    "    #remove accent char\n",
    "    row = unicodedata.normalize('NFKD', row).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "                 \n",
    "    #remove extra whitespace convert into a word  \n",
    "    row = row.strip()\n",
    "    \n",
    "    # TOKENIZATION: process of splitting text into smaller piece called tokens.\n",
    "    tokens = word_tokenize(row)\n",
    "    \n",
    "    # lemmatization step played -> play\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    row = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    \n",
    "    #remove stop words such as \"a\", \"the\", \"is\"\n",
    "    tokens = word_tokenize(row)\n",
    "    row = ' '.join([i for i in tokens if not i in stopWords])\n",
    "     \n",
    "    return row\n",
    "\n",
    "#normalizeText(\"Better advices. This isn't that cool. Next part could be.\")\n",
    "\n",
    "#run the column through the normalizing function\n",
    "df_HandC[\"Plot\"] = df_HandC[\"Plot\"].apply(normalizeText) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "30Jx7UPDIMmN",
    "outputId": "18cdbc95-bf9a-4d30-e946-697c1ccdd068"
   },
   "outputs": [],
   "source": [
    "##Prabesh being stupid. might be helpful?\n",
    "\n",
    "#vectorizer = TfidfVectorizer()\n",
    "#tfidf = vectorizer.fit_transform(df_HandC['Plot'])\n",
    "#print similarity matrix\n",
    "#print((tfidf*tfidf.T).A)\n",
    "\n",
    "# Make list from df for \"Plot\" column\n",
    "# Param: the df to use\n",
    "from itertools import chain\n",
    "def dfToList(df):\n",
    "    lst = []\n",
    "    for i, rows in df.iterrows():\n",
    "        l = [rows.Plot]\n",
    "        lst.append(l)\n",
    "    return list(chain.from_iterable(lst))\n",
    "\n",
    "# tf-idf vectorisation\n",
    "# Param: all plots from the df as a list (ie [plot1, plot2,...])\n",
    "def tfidfVec(plots):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(plots)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = tfidf.todense()\n",
    "    df_tfidf = pd.DataFrame(dense.tolist(), columns=feature_names)\n",
    "    return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accompanied</th>\n",
       "      <th>accompanying</th>\n",
       "      <th>actually</th>\n",
       "      <th>advises</th>\n",
       "      <th>advocating</th>\n",
       "      <th>africa</th>\n",
       "      <th>afterwards</th>\n",
       "      <th>aggressive</th>\n",
       "      <th>airlift</th>\n",
       "      <th>airport</th>\n",
       "      <th>...</th>\n",
       "      <th>would</th>\n",
       "      <th>wound</th>\n",
       "      <th>written</th>\n",
       "      <th>year</th>\n",
       "      <th>yell</th>\n",
       "      <th>yield</th>\n",
       "      <th>york</th>\n",
       "      <th>younger</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.13365</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047713</td>\n",
       "      <td>0.023856</td>\n",
       "      <td>0.023856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023856</td>\n",
       "      <td>0.023856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038494</td>\n",
       "      <td>0.023856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023856</td>\n",
       "      <td>0.023856</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.023856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028216</td>\n",
       "      <td>0.028216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028216</td>\n",
       "      <td>0.045529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accompanied  accompanying  actually   advises  advocating    africa  \\\n",
       "0     0.000000      0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "1     0.000000      0.000000  0.000000  0.041333    0.000000  0.000000   \n",
       "2     0.047713      0.023856  0.023856  0.000000    0.000000  0.023856   \n",
       "3     0.000000      0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "4     0.000000      0.000000  0.000000  0.000000    0.028216  0.000000   \n",
       "\n",
       "   afterwards  aggressive   airlift   airport  ...     would     wound  \\\n",
       "0    0.000000    0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "1    0.000000    0.000000  0.000000  0.000000  ...  0.066694  0.000000   \n",
       "2    0.000000    0.000000  0.023856  0.023856  ...  0.000000  0.047713   \n",
       "3    0.000000    0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "4    0.028216    0.028216  0.000000  0.000000  ...  0.022765  0.000000   \n",
       "\n",
       "    written      year      yell     yield      york   younger   zombie  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.13365   \n",
       "1  0.000000  0.000000  0.000000  0.041333  0.000000  0.000000  0.00000   \n",
       "2  0.000000  0.038494  0.023856  0.000000  0.023856  0.023856  0.00000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "4  0.028216  0.045529  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "\n",
       "       zone  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.023856  \n",
       "3  0.000000  \n",
       "4  0.000000  \n",
       "\n",
       "[5 rows x 905 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test showing how it works on first 5 elements of df_HandC\n",
    "tfidfVec(dfToList(df_HandC.head()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COGS118B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
